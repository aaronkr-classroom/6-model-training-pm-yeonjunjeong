{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTVn9m_nYjKs"
      },
      "source": [
        "# 연습문제 해답"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2P1MIK8YjKs"
      },
      "source": [
        "## 12. 조기 종료를 사용한 배치 경사 하강법으로 소프트맥스 회귀 구현하기\n",
        "문제: _사이킷런을 사용하지 않고 넘파이만으로 조기 종료를 사용한 배치 경사 하강법으로 소프트맥스 회귀를 구현해 보세요. 이를 붓꽃 데이터셋과 같은 분류 작업에 사용해 보세요._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdydHlgXYjKs"
      },
      "source": [
        "데이터를 로드하는 것부터 시작하겠습니다. 앞서 로드한 붓꽃 데이터셋을 재사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iris 데이터셋 다운로드하기\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris(as_frame=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "txtyW7GzYjKs"
      },
      "outputs": [],
      "source": [
        "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
        "y = iris[\"target\"].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdgX_LO3YjKs"
      },
      "source": [
        "모든 샘플에 대해 편향 항을 추가해야 합니다($x_0 = 1$). 이 작업을 수행하는 가장 쉬운 옵션은 사이킷런의 `add_dummy_feature()` 함수를 사용하는 것이지만, 이 연습의 요점은 알고리즘을 수동으로 구현하여 알고리즘을 더 잘 이해하도록 하는 것입니다. 그래서 한 가지 가능한 구현은 다음과 같습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "B8NGB8vxYjKs"
      },
      "outputs": [],
      "source": [
        "X_with_bias = np.c_[np.ones(len(X)), X]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQMEOVBsYjKs"
      },
      "source": [
        "데이터셋을 훈련 세트, 검증 세트 및 테스트 세트로 분할하는 가장 쉬운 옵션은 사이킷런의 `train_test_split()` 함수를 사용하는 것이지만, 이 역시 수동으로 수행합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "JKWCkwzUYjKs"
      },
      "outputs": [],
      "source": [
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "np.random.seed(42)\n",
        "rnd_indices = np.random.permutation(total_size)\n",
        "\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = y[rnd_indices[:train_size]]\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = y[rnd_indices[train_size:-test_size]]\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = y[rnd_indices[-test_size:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA1NQzeKYjKs"
      },
      "source": [
        "현재 타깃은 클래스 인덱스(0, 1 또는 2)이지만, 소프트맥스 회귀 모델을 훈련하려면 타깃 클래스 확률이 필요합니다. 각 샘플은 확률이 1.0인 타깃 클래스를 제외한 모든 클래스에 대해 0.0인 타깃 클래스 확률을 갖습니다(즉, 주어진 샘플에 대한 클래스 확률 벡터는 원-핫 벡터입니다). 클래스 인덱스 벡터를 각 샘플에 대한 원-핫 벡터를 포함하는 행렬로 변환하는 작은 함수를 작성해 보겠습니다. 이 코드를 이해하려면 `np.diag(np.ones(n))`가 주 대각선의 1을 제외한 0으로 가득 찬 n×n 행렬을 생성한다는 사실을 알아야 합니다. 또한 `a`가 넘파이 배열인 경우 `a[[1, 3, 2]]`는 `a[1]`, `a[3]` 및 `a[2]`와 동일한 3개의 행을 가진 배열을 반환합니다(이것이 [고급 넘파이 인덱싱](https://numpy.org/doc/stable/user/basics.indexing.html#advanced-indexing)입니다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "v4a43nAAYjKs"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y):\n",
        "    return np.diag(np.ones(y.max() + 1))[y]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK5sHTQdYjKt"
      },
      "source": [
        "처음 10개의 샘플에서 이 함수를 테스트해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaTiNfu4YjKt",
        "outputId": "7e09041b-b9b7-4087-c902-b7465d075b73"
      },
      "outputs": [],
      "source": [
        "y_train[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcMgx_lrYjKt",
        "outputId": "bedce8c8-ce19-431e-ced4-409996e377a8"
      },
      "outputs": [],
      "source": [
        "to_one_hot(y_train[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwgqIhRvYjKt"
      },
      "source": [
        "이제 훈련 세트와 테스트 세트에 대한 타깃 클래스 확률 행렬을 생성해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "WkdtS7hcYjKt"
      },
      "outputs": [],
      "source": [
        "Y_train_one_hot = to_one_hot(y_train)\n",
        "Y_valid_one_hot = to_one_hot(y_valid)\n",
        "Y_test_one_hot = to_one_hot(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82kPhkh7YjKu"
      },
      "source": [
        "이제 입력의 스케일을 조정해 보겠습니다. 훈련 세트에서 (편향은 제외한) 각 특성의 평균과 표준 편차를 계산한 다음 훈련 세트, 검증 세트 및 테스트 세트에서 각 특성을 중앙에 맞추고 스케일을 조정합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "zIxZvKABYjKu"
      },
      "outputs": [],
      "source": [
        "mean = X_train[:, 1:].mean(axis=0)\n",
        "std = X_train[:, 1:].std(axis=0)\n",
        "X_train[:, 1:] = (X_train[:, 1:] - mean) / std\n",
        "X_valid[:, 1:] = (X_valid[:, 1:] - mean) / std\n",
        "X_test[:, 1:] = (X_test[:, 1:] - mean) / std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln3zV_1rYjKu"
      },
      "source": [
        "이제 소프트맥스 함수를 구현해 보겠습니다. 다음 방정식으로 정의된다는 것을 기억하세요:\n",
        "\n",
        "$\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "fJl5rSjqYjKv"
      },
      "outputs": [],
      "source": [
        "def softmax(logits):\n",
        "    exps = np.exp(logits)\n",
        "    exp_sums = exps.sum(axis=1, keepdims=True)\n",
        "    return exps / exp_sums"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsNrCvEQYjKv"
      },
      "source": [
        "훈련을 시작할 준비가 거의 다 되었습니다. 입력과 출력의 수를 정의해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "JW0HPHbrYjKv"
      },
      "outputs": [],
      "source": [
        "n_inputs = X_train.shape[1]  # == 3 (2개의 특성과 편향)\n",
        "n_outputs = len(np.unique(y_train))  # == 3 (3개의 붓꽃 클래스)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unRzlR-eYjKv"
      },
      "source": [
        "이제 가장 어려운 부분인 훈련입니다! 이론적으로는 간단합니다. 수학 방정식을 파이썬 코드로 변환하기만 하면 됩니다. 하지만 실제로는 상당히 까다로울 수 있습니다. 특히 항의 순서나 인덱스가 혼동되기 쉽습니다. 심지어 제대로 작동하는 것처럼 보이지만 실제로는 정확히 계산되지 않는 코드가 나올 수도 있습니다. 확실하지 않은 경우 방정식의 각 항의 크기를 적어두고 코드의 해당 항이 정확히 일치하는지 확인해야 합니다. 각 항을 독립적으로 평가하고 인쇄하는 것도 도움이 될 수 있습니다. 좋은 소식은 이 모든 것이 사이킷런에 의해 잘 구현되어 있기 때문에 매일 이 작업을 수행할 필요는 없지만, 내부에서 무슨 일이 일어나고 있는지 이해하는 데 도움이 된다는 것입니다.\n",
        "\n",
        "따라서 우리에게 필요한 방정식은 비용 함수입니다:\n",
        "\n",
        "$J(\\mathbf{\\Theta}) =\n",
        "- \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$\n",
        "\n",
        "그리고 그레이디언트 방정식입니다:\n",
        "\n",
        "$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$\n",
        "\n",
        "$\\hat{p}_k^{(i)} = 0$일 때 $\\log\\left(\\hat{p}_k^{(i)}\\right)$는 계산할 수 없습니다. 따라서 $\\log\\left(\\hat{p}_k^{(i)}\\right)$에 작은 값 $\\epsilon$을 더해 `nan` 값을 피합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTQNKripYjKv",
        "outputId": "4f99cd15-e63d-4581-f73d-c56318523eb6"
      },
      "outputs": [],
      "source": [
        "eta = 0.5\n",
        "n_epochs = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-5\n",
        "\n",
        "np.random.seed(42)\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    logits = X_train @ Theta\n",
        "    Y_proba = softmax(logits)\n",
        "    if epoch % 1000 == 0:\n",
        "        Y_proba_valid = softmax(X_valid @ Theta)\n",
        "        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n",
        "        print(epoch, xentropy_losses.sum(axis=1).mean())\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1 / m * X_train.T @ error\n",
        "    Theta = Theta - eta * gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85KPnUh_YjKv"
      },
      "source": [
        "이게 다입니다! 소프트맥스 모델이 학습되었습니다. 모델 파라미터를 살펴봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFHteRC9YjKv",
        "outputId": "c4050dc2-5670-4637-b5a4-e38cbb0ede16"
      },
      "outputs": [],
      "source": [
        "Theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUC7LEL_YjKv"
      },
      "source": [
        "검증 세트에 대한 예측을 수행하고 정확도 점수를 확인해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDoeUeHPYjKv",
        "outputId": "c5dd6381-ac95-4693-a18e-886c2efb7ff4"
      },
      "outputs": [],
      "source": [
        "logits = X_valid @ Theta\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = Y_proba.argmax(axis=1)\n",
        "\n",
        "accuracy_score = (y_predict == y_valid).mean()\n",
        "accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqV3hkWXYjKw"
      },
      "source": [
        "이 모델은 꽤 괜찮아 보입니다. 연습을 위해 약간의 $\\ell_2$ 정규화를 추가해 보겠습니다. 다음 훈련 코드는 위의 코드와 유사하지만 손실에 $\\ell_2$ 페널티가 추가되고 그레이디언트가 적절한 추가 항을 가집니다(`Theta`의 첫 번째 원소는 편향에 해당하므로 규제하지 않음). 또한 학습률 `eta`를 높여 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BO8RDNfYjKw",
        "outputId": "6700f000-4ba7-4ab2-e54b-32a6818a12d5"
      },
      "outputs": [],
      "source": [
        "eta = 0.5\n",
        "n_epochs = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-5\n",
        "alpha = 0.01  # 규제 하이퍼파라미터\n",
        "\n",
        "np.random.seed(42)\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    logits = X_train @ Theta\n",
        "    Y_proba = softmax(logits)\n",
        "    if epoch % 1000 == 0:\n",
        "        Y_proba_valid = softmax(X_valid @ Theta)\n",
        "        xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n",
        "        l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n",
        "        total_loss = xentropy_losses.sum(axis=1).mean() + alpha * l2_loss\n",
        "        print(epoch, total_loss.round(4))\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1 / m * X_train.T @ error\n",
        "    gradients += np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
        "    Theta = Theta - eta * gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK8C9lCIYjKw"
      },
      "source": [
        "추가 $\\ell_2$ 페널티로 인해 손실이 이전보다 더 커 보이지만 이 모델이 더 나은 성과를 낼 수 있을까요? 알아봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqkpdo_DYjKw",
        "outputId": "ec7a8797-83d9-4ae1-b188-0520929a3ab5"
      },
      "outputs": [],
      "source": [
        "logits = X_valid @ Theta\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = Y_proba.argmax(axis=1)\n",
        "\n",
        "accuracy_score = (y_predict == y_valid).mean()\n",
        "accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF9LSdnpYjKw"
      },
      "source": [
        "이 경우 $\\ell_2$ 페널티로 인해 테스트 정확도가 변경되지 않았습니다. `alpha`를 미세 튜닝해 볼 수 있을까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GaHwj0-YjKw"
      },
      "source": [
        "이제 조기 종료를 추가해 보겠습니다. 이를 위해 모든 반복에서 검증 세트의 손실을 측정하고 오차가 증가하기 시작하면 중지하기만 하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_ItKkYRYjKw",
        "outputId": "2e76824d-7a64-48c2-ad4c-ca745f24b96c"
      },
      "outputs": [],
      "source": [
        "eta = 0.5\n",
        "n_epochs = 50_001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-5\n",
        "C = 100  # 규제 하이퍼파라미터\n",
        "best_loss = np.infty\n",
        "\n",
        "np.random.seed(42)\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    logits = X_train @ Theta\n",
        "    Y_proba = softmax(logits)\n",
        "    Y_proba_valid = softmax(X_valid @ Theta)\n",
        "    xentropy_losses = -(Y_valid_one_hot * np.log(Y_proba_valid + epsilon))\n",
        "    l2_loss = 1 / 2 * (Theta[1:] ** 2).sum()\n",
        "    total_loss = xentropy_losses.sum(axis=1).mean() + 1 / C * l2_loss\n",
        "    if epoch % 1000 == 0:\n",
        "        print(epoch, total_loss.round(4))\n",
        "    if total_loss < best_loss:\n",
        "        best_loss = total_loss\n",
        "    else:\n",
        "        print(epoch - 1, best_loss.round(4))\n",
        "        print(epoch, total_loss.round(4), \"조기 종료!\")\n",
        "        break\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1 / m * X_train.T @ error\n",
        "    gradients += np.r_[np.zeros([1, n_outputs]), 1 / C * Theta[1:]]\n",
        "    Theta = Theta - eta * gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZcF6-0gYjKw",
        "outputId": "a689e6ba-77c8-463c-b011-2b6350bbe0cd"
      },
      "outputs": [],
      "source": [
        "logits = X_valid @ Theta\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = Y_proba.argmax(axis=1)\n",
        "\n",
        "accuracy_score = (y_predict == y_valid).mean()\n",
        "accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvlhR1V_YjKw"
      },
      "source": [
        "여전히 검증 정확도에는 변화가 없지만, 적어도 조기 중지로 인해 훈련 시간이 조금 단축되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfNyoZ6zYjKx"
      },
      "source": [
        "이제 전체 데이터셋에 대한 모델의 예측을 그래프로 그려 보겠습니다(모델에 주입하는 모든 특성의 크기를 조정하는 것을 잊지 마세요):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "bxvmS0hwYjKx",
        "outputId": "aebd73a0-6482-4790-a6bf-93b033dd68b0"
      },
      "outputs": [],
      "source": [
        "custom_cmap = mpl.colors.ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\n",
        "\n",
        "x0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n",
        "                     np.linspace(0, 3.5, 200).reshape(-1, 1))\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "X_new = (X_new - mean) / std\n",
        "X_new_with_bias = np.c_[np.ones(len(X_new)), X_new]\n",
        "\n",
        "logits = X_new_with_bias @ Theta\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = Y_proba.argmax(axis=1)\n",
        "\n",
        "zz1 = Y_proba[:, 1].reshape(x0.shape)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\n",
        "plt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\n",
        "plt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n",
        "\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "contour = plt.contour(x0, x1, zz1, cmap=\"hot\")\n",
        "plt.clabel(contour, inline=1)\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.ylabel(\"Petal width\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.axis([0, 7, 0, 3.5])\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f4Vuqs5YjKx"
      },
      "source": [
        "이제 테스트 세트에서 최종 모델의 정확도를 측정해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tx47fXZYjKx",
        "outputId": "32884ee0-3c61-4c9d-b050-4668d4891e12"
      },
      "outputs": [],
      "source": [
        "logits = X_test @ Theta\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = Y_proba.argmax(axis=1)\n",
        "\n",
        "accuracy_score = (y_predict == y_test).mean()\n",
        "accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJYzi0Q4YjKx"
      },
      "source": [
        "테스트 세트에서 더 나은 성능을 얻었습니다. 이러한 변동성은 데이터셋의 크기가 매우 작기 때문일 수 있습니다. 훈련 세트, 검증 세트 및 테스트 세트의 샘플링 방법에 따라 상당히 다른 결과를 얻을 수 있습니다. 랜덤 시드를 변경하고 코드를 몇 번 다시 실행해 보면 결과가 달라지는 것을 확인할 수 있습니다."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
